{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230d891b",
   "metadata": {},
   "source": [
    "# Exercise#2 : Question-Answering Task\n",
    "\n",
    "Using a Bert model fine tuned for Q&A on Squad 2,0 dataset.\n",
    "\n",
    "https://huggingface.co/distilbert-base-cased-distilled-squad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19222f37-618c-4dab-9de6-0577076fc53b",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3a1e57-4252-4089-b3b2-2273c26dbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788be9aa-078c-4578-9107-cd3f9521e0c9",
   "metadata": {},
   "source": [
    "### Import appropriate model & config classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579cb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForQuestionAnswering\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import getpass\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e0f3f",
   "metadata": {},
   "source": [
    "## 1. Auto Model Class for task\n",
    "\n",
    "AutoModelForQuestionAnswering\n",
    "\n",
    "https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "\n",
    "Model in use\n",
    "\n",
    "https://huggingface.co/distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27b6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in use\n",
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "# Create tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634557f",
   "metadata": {},
   "source": [
    "## 2. Setup context and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069369be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the context and questions\n",
    "\n",
    "context = \"\"\"Beyonce Giselle Knowles-Carter (/biː\\ˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) \n",
    "is an American singer, songwriter, record producer and actress. Born and raised in Houston, \n",
    "Texas, she performed in various singing and dancing competitions as a child, and rose \n",
    "to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her \n",
    "father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of \n",
    "all time. Their hiatus saw the release of Beyonce\\'s debut album, Dangerously in Love (2003), \n",
    "which established her as a solo artist worldwide, earned five Grammy Awards and featured the \n",
    "Billboard Hot 100 number-one singles \\\"Crazy in Love\\\" and \\\"Baby Boy.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "   \"When did Beyonce start becoming popular?\",\n",
    "   \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
    "   \"What areas did Beyonce compete in when she was growing up?\",\n",
    "   \"In what city and state did Beyonce grow up?\",\n",
    "   \"In what R&B group was she the lead singer?\"\n",
    "]\n",
    "\n",
    "## Demonstrates that model understands the context and is not just using pattern matching\n",
    "## Uncomment to try these out !!!\n",
    "# context = \"two plus two is four. 6 plus 2 is eight. 1 plus nine is 10.\"\n",
    "# questions = [\"what is 2 + 2\", \"what is 6 + 2\", \"what is two + 1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d68dd7",
   "metadata": {},
   "source": [
    "## 3. Tokenize, and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647f0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try for any question by changing index\n",
    "index = 1\n",
    "inputs = tokenizer(questions[index], context, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Printhe tokenized string\n",
    "# print(inputs)\n",
    "# print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "# print(type(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30a8d3",
   "metadata": {},
   "source": [
    "## 4. Interpret the output\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput\n",
    "    \n",
    "Output has the following tensors:\n",
    "\n",
    "* **start_logits** A score for each token in the context. The token with the maximum score is the start token for the answer\n",
    "\n",
    "* **end_logits** A score for each token in the context. The token with the maximum score is the start token for the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585d1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start index:  tensor(149)   Token at start_index:  2003\n",
      "End index:  tensor(150)   Token at end_index:  2003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2003'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logits in the QuestionAnsweringOutput class are start/end of response\n",
    "# LINK\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# print(start_scores)\n",
    "# print(end_scores)\n",
    "\n",
    "# Get the start index = index for token that has the maximum score\n",
    "start_index = torch.argmax(start_scores)\n",
    "\n",
    "# This is to clarify the idea behind end index\n",
    "print(\"Start index: \", start_index, \"  Token at start_index: \", tokenizer.decode(inputs['input_ids'][0][start_index]))\n",
    "\n",
    "# Get the end index = index for the token that has the maximum score\n",
    "# Add 1 to include the last token\n",
    "end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "# This is to clarify the idea behind end index\n",
    "print(\"End index: \", end_index, \"  Token at end_index: \", tokenizer.decode(inputs['input_ids'][0][end_index-1]))\n",
    "\n",
    "# Decode the tokens between start_index & end_index\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0, start_index:end_index])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea7274",
   "metadata": {},
   "source": [
    "# Using pipeline\n",
    "\n",
    "An instance of QuestionAnsweringPipeline is used. Refer to documentation for parameters.\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89008fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f680c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '2003', score: 0.9859, start: 532, end: 536\n"
     ]
    }
   ],
   "source": [
    "question_answerer_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "\n",
    "result = question_answerer_pipeline(question=questions[1],     context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50cf68a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9425580501556396, 'start': 93, 'end': 95, 'answer': '13'}\n",
      "{'score': 0.8635509014129639, 'start': 10, 'end': 21, 'answer': '176 billion'}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/distilbert-base-cased-distilled-squad\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline\n",
    "output = question_answerer(question=question, context=context)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n",
    "question = \"How many parameters does BLOOM have?\"\n",
    "output = question_answerer(question=question, context=context)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b9807",
   "metadata": {},
   "source": [
    "# Using Inference Client\n",
    "\n",
    "Use the question_answering(...) function in InferenceClient class\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/v0.20.2/en/package_reference/inference_client#huggingface_hub.InferenceClient.question_answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad9c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8342d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy/paste HuggingFace token and hit <enter>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# You will prompted for the HuggingFace token\n",
    "print(\"Copy/paste HuggingFace token and hit <enter>\")\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass.getpass()\n",
    "\n",
    "client = InferenceClient(model=model_name, token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2aaf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringOutputElement(answer='176 billion', end=21, score=0.8635504245758057, start=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.question_answering(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbb5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
