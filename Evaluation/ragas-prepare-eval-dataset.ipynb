{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0af15c7-e0a5-4770-a1a9-671d23cca360",
   "metadata": {},
   "source": [
    "# Retriever & RAG evaluation with RAGAS\n",
    "\n",
    "Part-1:\n",
    "\n",
    "1. Chunk PDF file\n",
    "2. Create a vector database\n",
    "\n",
    "Part-2:\n",
    "\n",
    "3. Setup RAG pipeline\n",
    "4. Generate responses for test inputs\n",
    "5. Create eval dataset & save to file system\n",
    "\n",
    "\n",
    "Amazon 10-k filing\n",
    "https://ir.aboutamazon.com/sec-filings/default.aspxd\n",
    "\n",
    "#### Dependencies\n",
    "!pip install sentence-transformers chromad f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04894e02-2624-472d-9388-71406894fe39",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22281ca8-a437-4dd1-a658-4e26c272578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_metadata(result_docs):\n",
    "#     for result in result_docs:\n",
    "#         print(result.metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c88e8-6060-46a9-8cef-8037e43d064c",
   "metadata": {},
   "source": [
    "## Part-1 : Retriever : ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd0bef-7909-4b13-a032-f36c2d3ebd24",
   "metadata": {},
   "source": [
    "### 1. Load document and chunk\n",
    "\n",
    "Use the PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f63468-940b-4551-9b8e-bd671a065fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Reads the PDF and generates the chunks of specified size & overlaps\n",
    "def   create_chunks_from_pdf(pdf_source, chunk_size, chunk_overlap):\n",
    "# Change this \n",
    "# pdf_source = \"octank_financial_10K.pdf\"\n",
    "# pdf_source = \"amz-10k/amz-10k-2024.pdf\"\n",
    "\n",
    "    # Used the PDF loader\n",
    "    pdf_loader = PyPDFLoader(pdf_source) \n",
    "\n",
    "    # Load the documents\n",
    "    documents = pdf_loader.load()\n",
    "\n",
    "    # Text splitter\n",
    "    pdf_text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        keep_separator = False,\n",
    "        strip_whitespace = True,\n",
    "    )\n",
    "\n",
    "    # Chunked docs\n",
    "    chunked_documents = pdf_text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26d8d2-8608-491e-bc05-562cc56e3c3a",
   "metadata": {},
   "source": [
    "### 2. ChromaDB Vectorstore : Generate the embeddings for the chunks & add \n",
    "\n",
    "In this sample we are using ChromaDB, but you may use any vector store.\n",
    "\n",
    "https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "\n",
    "\n",
    "\n",
    "#### Parameters (check doc for additional parameters:\n",
    "https://api.python.langchain.com/en/stable/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#langchain_community.vectorstores.chroma.Chroma\n",
    "\n",
    "* collection_name (required)\n",
    "* embedding_function (optional)\n",
    "* persist_directory (optional)\n",
    "* collection_metadata (optional)\n",
    "* client (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2b4496-f935-49dc-9041-6331fab20855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def setup_chroma_db(embedding_function, chunked_documents, collection_name, metadata):\n",
    "\n",
    "    # load it into Chroma using default embedding all-MiniLM-L6-v2 \n",
    "    \n",
    "    \n",
    "    vector_store = Chroma(collection_name=collection_name, collection_metadata=collection_metadata, embedding_function=embedding_function)\n",
    "    vector_store.add_documents(chunked_documents)\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a7695-400a-4b6f-8bb6-c23f43a1668f",
   "metadata": {},
   "source": [
    "## Part-2 : RAG pipeline for processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545be039-0315-40d9-be81-3052bc48ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "import sys\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892f699-1e7a-451d-afd9-abd5a2b507a3",
   "metadata": {},
   "source": [
    "### 3. Setup RAG pipeline\n",
    "\n",
    "The RAG pipeline will be used for generating the response(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80b5a2b-d82b-492c-afb3-79b3554cb2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "def   setup_rag_pipeline(rag_llm):\n",
    "    # Setup a prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" You are a smart agent who uses only the provided provided context to answer the given question. \n",
    "                     Your answers are concise and to the point.\n",
    "                     Keep your responses under 1000 characters in length.\n",
    "                     \\n\\n Question: \\n {question} \n",
    "                     \\n\\n Context: \\n {context}\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\", \"context\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Setup RAG pipeline\n",
    "    rag_pipeline = prompt | rag_llm\n",
    "\n",
    "    return rag_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6413f3e-c67f-49b2-88ee-0084893039c4",
   "metadata": {},
   "source": [
    "### 4. Generate responses for test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b16edb5-5125-46aa-8c2e-6e59d8d52e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def   split_eval_input_reference(user_input_reference_file_path):\n",
    "    user_input = []\n",
    "    reference = []\n",
    "    \n",
    "    with open(user_input_reference_file_path, 'r', encoding='utf-8') as f:  # Handle potential encoding issues\n",
    "            data = json.load(f)\n",
    "    \n",
    "            for item in data:\n",
    "                user_input.append(item.get(\"user_input\")) #Use .get to handle missing keys gracefully\n",
    "                reference.append(item.get(\"reference\"))   #Use .get to handle missing keys gracefully\n",
    "\n",
    "    return user_input, reference\n",
    "\n",
    "# Read the user_input_reference file and split it into 2 arrays\n",
    "# filepath = \"amz-10k/amz-10k-2024-QA.json\"\n",
    "# user_input, reference = split_eval_input_reference(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888ad731-826e-4116-be1b-8b0303cac289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_from_docs(result_docs):\n",
    "    context=''\n",
    "    for result in result_docs:\n",
    "       context = context + \"\\n\" + result.page_content\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273858c5-212f-4ebd-86c6-9e4328be1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def     generate_context_response_evaluation_dataset(k, user_input, reference):\n",
    "    # Now gather the contexts and responses for the user_input\n",
    "    retrieved_contexts = []\n",
    "    responses = []\n",
    "    \n",
    "    for question in user_input:\n",
    "        print(\"RAG pipeline processing: \", question)\n",
    "        \n",
    "        result_docs = vector_store.similarity_search(question, k = k)\n",
    "        # result_docs = vector_store.max_marginal_relevance_search(question, k = k, lambda_mult = 0.5)\n",
    "        \n",
    "        context = create_context_from_docs(result_docs)\n",
    "    \n",
    "        # Create a list of contexts retrieved from the vector store\n",
    "        context_list = []\n",
    "        for doc in result_docs:\n",
    "            context_list.append(doc.page_content)\n",
    "        \n",
    "        retrieved_contexts.append(context_list)\n",
    "        response = rag_pipeline.invoke(input={\"question\": question, \"context\" : context})\n",
    "        responses.append(response)\n",
    "    \n",
    "    # Create the evaluation dataset\n",
    "    eval_dataset = {\n",
    "        \"user_input\" : user_input,\n",
    "        \"retrieved_contexts\"  : retrieved_contexts,\n",
    "        \"reference\" : reference,\n",
    "        \"response\" : responses\n",
    "    }\n",
    "\n",
    "    return eval_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027812d9-eee7-4afa-96bc-5bbe2ef1977b",
   "metadata": {},
   "source": [
    "## 5. Create eval dataset & save to file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b032875c-d85c-48b6-b3ae-e594a730f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks:  456\n",
      "RAG pipeline processing:  What are Amazon's primary revenue sources?\n",
      "RAG pipeline processing:  How much did Amazon's net sales grow in 2024?\n",
      "RAG pipeline processing:  What is the operating income for 2024?\n",
      "RAG pipeline processing:  How does Amazon manage its inventory risk?\n",
      "RAG pipeline processing:  What is the significance of AWS to Amazon's overall business?\n",
      "RAG pipeline processing:  How does Amazon handle cybersecurity risks?\n",
      "RAG pipeline processing:  What are Amazon's major operating expenses?\n",
      "RAG pipeline processing:  How does Amazon manage its cash flow?\n",
      "RAG pipeline processing:  What are Amazon's key risks in international operations?\n",
      "RAG pipeline processing:  How does Amazon address competition in its markets?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from utils.create_llm import create_gpt_llm, create_cohere_llm, create_ollama_llm, create_hugging_face_llm, create_google_llm, create_ai21_llm\n",
    "\n",
    "# Initialize the inputs\n",
    "pdf_source = \"amzn-10k/amz-10k-2024.pdf\"                          # Document to be indexed\n",
    "user_input_reference_file_path=\"amzn-10k/amz-10k-2024-QA.json\"    # A JSON file with user_input and reference data\n",
    "\n",
    "# Setup chunking\n",
    "chunk_size = 900\n",
    "chunk_overlap = 150\n",
    "k=5\n",
    "\n",
    "# 1. Chunk the docs\n",
    "chunked_documents = create_chunks_from_pdf(pdf_source, chunk_size, chunk_overlap)\n",
    "print(\"Number of chunks: \", len(chunked_documents))\n",
    "\n",
    "# 2. Create the vector store\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection_metadata = {'embedding': 'all-MiniLM-L6-v2', 'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n",
    "vector_store = setup_chroma_db(embedding_function, chunked_documents, \"financial-analysis\", collection_metadata)\n",
    "\n",
    "# 3. Setup the LLM for RAG pipeline\n",
    "rag_llm = create_cohere_llm(args={\"temperature\":0})\n",
    "rag_pipeline = setup_rag_pipeline(rag_llm)\n",
    "\n",
    "# 4. Read the eval input and references\n",
    "user_input, reference = split_eval_input_reference(user_input_reference_file_path)\n",
    "eval_dataset = generate_context_response_evaluation_dataset(k, user_input, reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3d67b0-ba17-4454-97e4-00bb91efedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset file :  amzn-10k/amz-10k-2024-eval-dataset-cohere-chunk900-overlap150-k5.json\n"
     ]
    }
   ],
   "source": [
    "# Output file used for evaluation\n",
    "eval_filepath = f\"amzn-10k/amz-10k-2024-eval-dataset-cohere-chunk{chunk_size}-overlap{chunk_overlap}-k{k}.json\" \n",
    "\n",
    "# Save the dataset to file system\n",
    "with open(eval_filepath, 'w', encoding='utf-8') as f:  # 'w' for writing, utf-8 encoding\n",
    "    json.dump(eval_dataset, f, indent=4, ensure_ascii=False)  # indent for pretty printing, ensure_ascii handles non-ASCII\n",
    "\n",
    "print(\"Created evaluation dataset file : \", eval_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
