{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cbb308-ed78-449e-94d8-976acb9bfc8d",
   "metadata": {},
   "source": [
    "# Evaluate libray\n",
    "\n",
    "Learn to use the HuggingFace evaluate library\n",
    "\n",
    "**Note** Library doesn't support the LLM-as-a-Judge metric\n",
    "\n",
    "https://huggingface.co/docs/evaluate/en/index\n",
    "\n",
    "**Classes**\n",
    "\n",
    "https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes\n",
    "\n",
    "https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/main_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16c702-8516-49be-940a-5a3e0c2ed77f",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5ba569-3b94-44f2-af54-0eae153bd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet evaluate, langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d21be-b49c-4329-94fb-204a211549de",
   "metadata": {},
   "source": [
    "## 1. Evaluate basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedc4164-3886-4ff4-a239-c5555c800462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy description:  \n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "Accuracy features:  {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "print(\"Accuracy description: \", evaluate.load(\"accuracy\").description)\n",
    "print(\"Accuracy features: \", evaluate.load(\"accuracy\").features)\n",
    "\n",
    "# print(\"Precision features: \", evaluate.load(\"precision\").features)\n",
    "# print(\"Precision features: \", evaluate.load(\"precision\").features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a481af-9687-4c05-82f9-c3de9a75ba34",
   "metadata": {},
   "source": [
    "### Compute single metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b111d14-e4c5-4b90-8b09-c97b1176409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(predictions=[1,0,1], references=[1,1,1])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20dbce5-e829-4b00-933e-96367a7ab4c8",
   "metadata": {},
   "source": [
    "### Compute multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e7a890-81f9-4ae9-9661-fcdf02cabf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "result = clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562457c-f369-4690-b017-3e59263a5b7d",
   "metadata": {},
   "source": [
    "## 2. Metric evaluation : Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6899c0b-1bf5-4116-bf86-fd8ad2e6bd76",
   "metadata": {},
   "source": [
    "### Setup Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d90ddd8-a09d-491a-8e32-0e5031fd6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d81d874-fb70-4186-a2f0-695b54c16a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm, create_hugging_face_chat_llm, create_groq_chat_llm\n",
    "from datasets import Dataset\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant that assigns a sentiment classification to the given input text. Provide your output as a single number representing the sentiment:\n",
    "\n",
    "1 for positive sentiment\n",
    "2 for negative sentiment\n",
    "0 for neutral sentiment\n",
    "Examples of expected output:\n",
    "Input: \"it was beautiful\"\n",
    "Output: 1\n",
    "\n",
    "Input: \"i did not like it\"\n",
    "Output: 2\n",
    "\n",
    "For each new input, output only the number corresponding to its sentiment, with no additional text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages({(\"system\", system), (\"human\", human)})\n",
    "\n",
    "# Utility function for creating the chain\n",
    "\n",
    "\n",
    "def setup_llm(model_name):\n",
    "    llm = create_groq_chat_llm(model_name=model_name, args={\"temperature\":0})\n",
    "    return llm\n",
    "    \n",
    "def setup_llm_chain(model_name):\n",
    "\n",
    "    llm = setup_llm(model_name)\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3a698-7ee7-4e36-afce-0436a4fe089d",
   "metadata": {},
   "source": [
    "### Setup test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34befa55-3f75-4b55-8538-0f75732bde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "  {\"text\": \"I love the new design of this website!\", \"reference\": 1},\n",
    "  {\"text\": \"The product stopped working after just two days.\", \"reference\": 2},\n",
    "  {\"text\": \"It’s an average movie, nothing special.\", \"reference\": 0},\n",
    "  {\"text\": \"The customer service was exceptional and helpful.\", \"reference\": 1},\n",
    "  {\"text\": \"I am extremely disappointed with this purchase.\", \"reference\": 2},\n",
    "  {\"text\": \"The presentation was neither good nor bad.\", \"reference\": 0},\n",
    "  {\"text\": \"This is the best book I have read this year!\", \"reference\": 1},\n",
    "  {\"text\": \"The food was cold and tasted terrible.\", \"reference\": 2},\n",
    "  {\"text\": \"I feel indifferent about the changes.\", \"reference\": 0},\n",
    "  {\"text\": \"The concert was a fantastic experience.\", \"reference\": 1}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3ed80e-88e8-45a9-89bc-242df154fa60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'I love the new design of this website!',\n",
       "  'reference': 1,\n",
       "  'prediction': 1},\n",
       " {'text': 'The product stopped working after just two days.',\n",
       "  'reference': 2,\n",
       "  'prediction': 1}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [\"mixtral-8x7b-32768\", \"llama3-8b-8192\"]\n",
    "model_index = 0\n",
    "\n",
    "model_name =model_names[model_index]\n",
    "\n",
    "chain = setup_llm_chain(model_name)\n",
    "\n",
    "for i, test in enumerate(test_dataset):\n",
    "    response = chain.invoke({\"text\": \"I am doing great.\"})\n",
    "    output = response.content\n",
    "    test_dataset[i]['prediction'] = int(output)\n",
    "\n",
    "test_dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4dd9aa-5efd-48f4-8821-097811dd03df",
   "metadata": {},
   "source": [
    "### Evaluate : Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e86bac3-cdd5-4137-ac0a-3921cc1e69f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the dataset (separate array for each column)\n",
    "flattened_data = Dataset.from_list(test_dataset)\n",
    "\n",
    "# Calculate accuracy\n",
    "result = accuracy.compute(predictions=flattened_data['prediction'], references=flattened_data['reference'])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d16d8-5e34-40e4-9054-3fc3ca160874",
   "metadata": {},
   "source": [
    "## 3. Task specific metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c568d7-7e35-42c7-9dca-877320775acb",
   "metadata": {},
   "source": [
    "### Checkout response format for Q&A pipeline/task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c541362-ca2a-4a25-9c28-12ba3c75dba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.2117149531841278,\n",
       " 'start': 59,\n",
       " 'end': 84,\n",
       " 'answer': 'gives freedom to the user'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "result = nlp(\n",
    "                question='Why is model conversion important?', \n",
    "                context='The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe6a07-b431-4b39-aa53-503b6377534d",
   "metadata": {},
   "source": [
    "### QuestionAnsweringEvaluator\n",
    "\n",
    "https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/evaluator/question_answering.py#L143\n",
    "\n",
    "https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ef2d1b-36ee-4396-9dfe-068c3702ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'exact': 60.0, 'f1': 60.0, 'total': 10, 'HasAns_exact': 100.0, 'HasAns_f1': 100.0, 'HasAns_total': 6, 'NoAns_exact': 0.0, 'NoAns_f1': 0.0, 'NoAns_total': 4, 'best_exact': 60.0, 'best_exact_thresh': 0.996135950088501, 'best_f1': 60.0, 'best_f1_thresh': 0.996135950088501, 'total_time_in_seconds': 1.6325096000218764, 'samples_per_second': 6.125538250964034, 'latency_in_seconds': 0.16325096000218764}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from evaluate import evaluator\n",
    "from evaluate import QuestionAnsweringEvaluator\n",
    "\n",
    "\n",
    "qa_evaluator = QuestionAnsweringEvaluator()\n",
    "\n",
    "qa_dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"validation[:10]\")\n",
    "\n",
    "results = qa_evaluator.compute(\n",
    "    model_or_pipeline='distilbert/distilbert-base-cased-distilled-squad',\n",
    "    data=qa_dataset,\n",
    "    metric=\"squad_v2\",\n",
    "    squad_v2_format=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff8cb3-aa9e-4c94-86ec-52bebb13e27d",
   "metadata": {},
   "source": [
    "### Text Classification Evaluator\n",
    "\n",
    "https://huggingface.co/docs/evaluate/v0.4.0/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9bfde20-55b9-4e25-88f7-cdb7fa44e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import TextClassificationEvaluator\n",
    "\n",
    "text_classification_evaluator = TextClassificationEvaluator() # evaluator(\"text-classification\")\n",
    "\n",
    "model_index = 1\n",
    "model_name =model_names[model_index]\n",
    "\n",
    "llm = setup_llm(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82fca420-3ccc-4bf7-867c-8d07c11ada73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = [\n",
    "  {\"text\": \"I love the new design of this website!\", \"label\": 1.0},\n",
    "  {\"text\": \"The product stopped working after just two days.\", \"label\": 2.0},\n",
    "  {\"text\": \"It’s an average movie, nothing special.\", \"label\": 0.0},\n",
    "  {\"text\": \"The customer service was exceptional and helpful.\", \"label\": 1.0}\n",
    "]\n",
    "\n",
    "label_mapping={\"POSITIVE\": 1.0, \"NEGATIVE\": 2.0, \"NEUTRAL\": 0.0}\n",
    "\n",
    "test_dataset_flattened = Dataset.from_list(test_dataset)\n",
    "\n",
    "results = text_classification_evaluator.compute(\n",
    "    model_or_pipeline = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    data = test_dataset_flattened,\n",
    "    metric = \"accuracy\",\n",
    "    label_mapping=label_mapping,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd256f-7148-4032-a52e-5ca49446611d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0dda5-a217-4184-a34f-299cb31ab1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
