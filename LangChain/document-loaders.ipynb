{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1534e8-3bd1-47ac-a05c-bc3e7debc039",
   "metadata": {},
   "source": [
    "# LangChain Document Loaders\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders/html\n",
    "\n",
    "### Checkout list of loaders\n",
    "\n",
    "https://python.langchain.com/docs/integrations/document_loaders\n",
    "\n",
    "### Document class\n",
    "\n",
    "##### Document\n",
    "https://api.python.langchain.com/en/stable/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741367f3-05bf-4f00-8b51-0693ba72201b",
   "metadata": {},
   "source": [
    "## 1. PDF Loader\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
    "\n",
    "#### Dependency\n",
    "\n",
    "Install the pypdf packge.\n",
    "\n",
    "pip install pypdf\n",
    "\n",
    "#### API\n",
    "\n",
    "##### PyPDFLoader\n",
    "https://api.python.langchain.com/en/stable/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html#langchain_community.document_loaders.pdf.PyPDFLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8081f3-7546-460b-98bb-eceb4bb810e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315f6b6-17c5-4228-8a17-7758898045cc",
   "metadata": {},
   "source": [
    "### Load a PDF from arxiv.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5280ae4-0a5c-497c-8039-3b602475eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document list size :  43\n",
      "Metadata :  {'source': 'https://arxiv.org/pdf/2201.11903.pdf', 'page': 0}\n",
      "Page content : \n",
      "Chain-of-Thought Prompting Elicits Reasoning\n",
      "in Large Language Models\n",
      "Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\n",
      "Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\n",
      "Google Research, Brain Team\n",
      "{jasonwei,dennyzhou}@google.com\n",
      "Abstract\n",
      "We explore how generating a chain of thought —a series of intermediate reasoning\n",
      "steps—signiﬁcantly improves the ability of large language models to perform\n",
      "complex reasoning. In particular, we show how such reasoning abilities emerge\n",
      "naturally in sufﬁciently large language models via a simple method called chain-of-\n",
      "thought prompting , where a few chain of thought demonstrations are provided as\n",
      "exemplars in prompting.\n",
      "Experiments on three large language models show that chain-of-thought prompting\n",
      "improves performance on a range of arithmetic, commonsense, and symbolic\n",
      "reasoning tasks. The empirical gains can be striking. For instance, prompting a\n",
      "PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\n",
      "accuracy on the GSM8K benchmark of math word problems, surpassing even\n",
      "ﬁnetuned GPT-3 with a veriﬁer.\n",
      "A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.Chain-of-Thought PromptingQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.Standard Prompting\n",
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?Model Input\n",
      "Model OutputModel OutputModel Input\n",
      "Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\n",
      "commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\n",
      "36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6  [cs.CL]  10 Jan 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load from local file system or from a URL\n",
    "# You may also us the PDF web loader\n",
    "\n",
    "source_pdf = 'https://arxiv.org/pdf/2201.11903.pdf'\n",
    "\n",
    "pdf_loader = PyPDFLoader(source)\n",
    "\n",
    "# Read from local file system\n",
    "# pdf_loader = PyPDFLoader('c:/Users/raj/Downloads/arxiv-cot-2201.11903.pdf')\n",
    "\n",
    "# Loads pdf into a single document\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "print(\"Document list size : \", len(pdf_document))\n",
    "print(\"Metadata : \", pdf_document[0].metadata)\n",
    "print(\"Page content : \")\n",
    "print(pdf_document[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0d9c3b-26ef-4c57-8209-f286bf6a71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_and_split() Loads the PDF using PyPDF into a list of documents\n",
    "# PDF is chunked by page; each document represent a page with page number.\n",
    "pages = pdf_loader.load_and_split()\n",
    "\n",
    "print(\"Number of pages : \", len(pages))\n",
    "print(\"Metadata : \", pages[10].metadata)\n",
    "\n",
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d842df7-9802-4e33-b5b0-dd00c81a5ee9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 2. CSV Loader\n",
    "\n",
    "#### API\n",
    "\n",
    "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html#langchain_community.document_loaders.csv_loader.CSVLoader\n",
    "\n",
    "#### EV data\n",
    "Sample data for testing. Docwnload the file from link below:\n",
    "\n",
    "https://catalog.data.gov/dataset/electric-vehicle-population-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc86e05-3100-4cb8-9ca1-8fbc9d90b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages :  177866\n",
      "Metadata :  {'source': 'c:/Users/raj/Downloads/Electric_Vehicle_Population_Data.csv', 'row': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VIN (1-10): 5YJYGDEE1L\\nCounty: King\\nCity: Seattle\\nState: WA\\nPostal Code: 98122\\nModel Year: 2020\\nMake: TESLA\\nModel: MODEL Y\\nElectric Vehicle Type: Battery Electric Vehicle (BEV)\\nClean Alternative Fuel Vehicle (CAFV) Eligibility: Clean Alternative Fuel Vehicle Eligible\\nElectric Range: 291\\nBase MSRP: 0\\nLegislative District: 37\\nDOL Vehicle ID: 125701579\\nVehicle Location: POINT (-122.30839 47.610365)\\nElectric Utility: CITY OF SEATTLE - (WA)|CITY OF TACOMA - (WA)\\n2020 Census Tract: 53033007800'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CHANGE this\n",
    "ev_file_path = 'c:/Users/raj/Downloads/Electric_Vehicle_Population_Data.csv'\n",
    "\n",
    "csv_loader = CSVLoader(file_path=ev_file_path)\n",
    "\n",
    "csv_data = csv_loader.load()\n",
    "\n",
    "print(\"Number of rows : \", len(csv_data))\n",
    "print(\"Metadata : \", csv_data[10].metadata)\n",
    "\n",
    "csv_data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15ecfa-e1f1-48c1-a645-f8e5a201d118",
   "metadata": {},
   "source": [
    "## 3. URL Loader\n",
    "\n",
    "https://python.langchain.com/docs/integrations/document_loaders/url\n",
    "\n",
    "API\n",
    "\n",
    "##### UnstructuredURLLoader\n",
    "\n",
    "Use the unstructured partition function to detect the MIME type and route the file to the appropriate partitioner.\r\n",
    "\r\n",
    "You can run the loader in one of two modes: “single” and “elements”. If you use “single” mode, the document will be returned as a single langchain Document object. If you use “elements” mode, the unstructured library will split the document into elements such as Title and NarrativeText. You can pass in additional unstructured kwargs after mode to apply different unstructured settings.\n",
    "\n",
    "https://api.python.langchain.com/en/stable/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html#langchain_community.document_loaders.url.UnstructuredURLLoader\n",
    "##### WebLoaders\n",
    "\n",
    "\n",
    "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. \n",
    "\n",
    "https://python.langchain.com/docs/integrations/document_loaders/web_base\n",
    "\n",
    "API\n",
    "\n",
    "https://sj-langchain.readthedocs.io/en/latest/document_loaders/langchain.document_loaders.web_base.WebBaseLoader.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259cf88-729d-439d-9295-673a9c305109",
   "metadata": {},
   "source": [
    "#### Requirement\n",
    "\n",
    "##### URL Loader\n",
    "!pip install unstructured libmagic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de5d6ae-019b-43b2-bc2e-dcdc352841d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "urls = [\n",
    "        'https://en.wikipedia.org/wiki/Large_language_model',\n",
    "        'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)'\n",
    "       ]\n",
    "\n",
    "\n",
    "web_loader = WebBaseLoader(urls)\n",
    "\n",
    "# Returns a list of *Document* objects\n",
    "# Metadata has the {'source': '...', 'title': '...', 'language': '...}\n",
    "data = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53791dfb-3593-4d6f-b505-fdc6583194b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "377ea0b8-f76e-42a0-919d-3b7734de2ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://en.wikipedia.org/wiki/Large_language_model',\n",
       " 'title': 'Large language model - Wikipedia',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e238b9-aece-4ad1-a1fc-d6ac7e246b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = web_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "954af3e2-a802-4ab2-bbbf-f59a82c038e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b5c47-0ce2-4d1b-82d4-27dd399e0f1e",
   "metadata": {},
   "source": [
    "## 4. Wikipedia Loader\n",
    "\n",
    "https://python.langchain.com/docs/integrations/document_loaders/wikipedia\n",
    "\n",
    "#### API\n",
    "\n",
    "https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.wikipedia.WikipediaLoader.html#langchain_community.document_loaders.wikipedia.WikipediaLoader\n",
    "\n",
    "* query (str) – The query string to search on Wikipedia.\r\n",
    "* lang (str, optional) – The language code for the Wikipedia language edition. Defaults to “en”.\r\n",
    "* load_max_docs (int, optional) – The maximum number of documents to load. Defaults to 100.\r\n",
    "* load_all_available_meta (bool, optional) – Indicates whether to load all available metadata for each document. Defaults to False.\r\n",
    "* doc_content_chars_max (int, optional) – The maximum number of characters for the document content. Defaults to 4000.\r\n",
    "\r\n",
    "#### Note\r\n",
    "Requires the python wikipedia package\r\n",
    "\r\n",
    "\r\n",
    "!pip install --upgrade --quiet  wikipediauiet  wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23141f5a-8bf6-4c7d-8565-99b6ec7569f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_loader = WikipediaLoader(\"Large Language Models\", load_max_docs=2)\n",
    "\n",
    "wiki_docs = wiki_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f66008d-a4c7-4012-bd05-d558bf511257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of docs : \", len(wiki_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3432fa-8411-441c-b133-6ba7ce95ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI\\'s GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google\\'s PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI\\'s Grok, Meta\\'s LLaMA family of open-source models, Anthropic\\'s Claude models, and Mistral AI\\'s open source models.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}\n"
     ]
    }
   ],
   "source": [
    "print(wiki_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a7e368-40a0-4a5b-9219-16a8af566a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI's Grok, Meta's LLaMA family of open-source models, Anthropic's Claude models, and Mistral AI's open source models.\n",
      "\n",
      "\n",
      "== History ==\n",
      "At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 Seq2seq technology,  and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\n",
      "Although decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\n",
      "In the meantime, competing language models have for the most part been playing catch-up to the GPT series, at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google's 2019 T5-11B and 2022 PaLM-E, and Anthropic's 2024 Claude 3. In terms of Elo ratings, on January 26, 2024, Google's Bard (Gemini Pro) surpassed the regular GPT-4, but not the limited-availability GPT-4-Turbo.Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024, Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\n",
      "\n",
      "\n",
      "== Dataset preprocessing ==\n",
      "\n",
      "\n",
      "=== Probabilistic tokenization ===\n",
      "Because machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\n",
      "Probabilistic t\n"
     ]
    }
   ],
   "source": [
    "print(wiki_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31623ddc-5401-4c95-a0b5-e07e6c4b54fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
