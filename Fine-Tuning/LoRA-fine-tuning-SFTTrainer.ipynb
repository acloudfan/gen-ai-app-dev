{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdd9752-334e-42b4-ac61-c3f0ae3e5a65",
   "metadata": {},
   "source": [
    "# HuggingFace Supervised Fine-tuning Trainer (SFT)\n",
    "## LoRA Fine-tuning\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer\n",
    "\n",
    "## TinyLlamma\n",
    "https://arxiv.org/pdf/2401.02385\n",
    "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1\n",
    "https://huggingface.co/facebook/opt-350m\n",
    "https://huggingface.co/facebook/MobileLLM-125M\n",
    "\n",
    "## Example scripts\n",
    "https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py\n",
    "\n",
    "## Inspired by\n",
    "https://colab.research.google.com/github/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb\n",
    "\n",
    "https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4606f3-a8fd-47b2-8f09-7ba419be1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6313e545-c8ce-4d80-8707-e6c31bccf60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (1.2.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from peft) (0.26.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from torch>=1.13.0->peft) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354e58d2-84a8-43df-9cd1-52bd4ca35a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816005c1-8134-4026-9f43-b722b0e800b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Select the base model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Requires code to be executed for loading the model\n",
    "# model_name = \"facebook/MobileLLM-125M\"\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"fb-opt-350-ft\"\n",
    "os.environ[\"WANDB_DIR\"] = \"./temp\"\n",
    "os.environ[\"WANDB_JOB_NAME\"] = \"some-job-name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbab373-c83f-4902-b4d4-ffa2f10e5e47",
   "metadata": {},
   "source": [
    "## 1. Prepare the dataset\n",
    "\n",
    "**Dataset format support**\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adfa2555-5797-4015-8084-909f7e63150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"HuggingFaceTB/smoltalk\"\n",
    "dataset_split = \"everyday-conversations\"\n",
    "\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0b81e-df31-45b3-8176-6c3807918ef6",
   "metadata": {},
   "source": [
    "## 2. Load the model to appropriate available device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcad2466-9fd4-4fff-a976-b6d85984575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Check the machine in use and set the device to use for training\n",
    "# cuda = GPU, mps = Metal Performance Shaders on macOS or Apple GPU, cpu otherwise\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print device info\n",
    "print(\"Model loaded to: \", device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pretrained model & move it to the specified device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Setup for the model specific chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf3b85-9c9f-4edf-9796-3ac29d573819",
   "metadata": {},
   "source": [
    "## 3. Setup the training configuration\n",
    "\n",
    "**SFTConfig**\n",
    "\n",
    "https://huggingface.co/docs/trl/v0.12.2/en/sft_trainer#trl.SFTConfig\n",
    "\n",
    "This object specifies hyperparameters and settings for the fine-tuning process. It’s tailored to supervised fine-tuning tasks, often used for adapting language models to specific tasks or datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de727a69-4bc0-4338-b9bd-7fe526ce6257",
   "metadata": {},
   "source": [
    "### 3.1 PEFT configuration\n",
    "\n",
    "**LoraConfig**\n",
    "\n",
    "https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig\n",
    "\n",
    "**Task type**\n",
    "\n",
    "The `task_type` parameter in LoRA (Low-Rank Adaptation) and the `peft` library specifies the type of task for which the model is being fine-tuned. The following are the possible values for `task_type` based on the common use cases supported by the `peft` library:\r\n",
    "\r\n",
    "### **Possible Values for `task_type`**\r\n",
    "\r\n",
    "1. **`\"CAUSAL_LM\"`**\r\n",
    "   - **Description**: Fine-tuning for **causal language modeling** tasks, where the model predicts the next token in a sequence based on previous tokens. \r\n",
    "   - **Examples**: GPT-like autoregressive models.\r\n",
    "\r\n",
    "2. **`\"SEQ2SEQ_LM\"`**\r\n",
    "   - **Description**: Fine-tuning for **sequence-to-sequence language modeling** tasks, where the input and output are sequences. Used in tasks like translation or summarization.\r\n",
    "   - **Examples**: T5, BART.\r\n",
    "\r\n",
    "3. **`\"TOKEN_CLASSIFICATION\"`**\r\n",
    "   - **Description**: Fine-tuning for tasks where the goal is to classify tokens in the input sequence.\r\n",
    "   - **Examples**: Named Entity Recognition (NER), Part-of-Speech (POS) tagging.\r\n",
    "\r\n",
    "4. **`\"SEQ_CLASSIFICATION\"`**\r\n",
    "   - **Description**: Fine-tuning for **sequence classification** tasks, where the entire sequence is classified into categories.\r\n",
    "   - **Examples**: Sentiment analysis, text classification.\r\n",
    "\r\n",
    "5. **`\"MULTIPLE_CHOICE\"`**\r\n",
    "   - **Description**: Fine-tuning for tasks involving selecting the correct choice from multiple options.\r\n",
    "   - **Examples**: Tasks like the SWAG dataset or common-sense reasoning.\r\n",
    "\r\n",
    "6. **`\"QUESTION_ANSWERING\"`**\r\n",
    "   - **Description**: Fine-tuning for tasks where the model predicts an answer span from the input context and question.\r\n",
    "   - **Examples**: SQuAD dataset.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **How `task_type` Influences Model Behavior**\r\n",
    "The `task_type` parameter guides the integration of LoRA layers and optimizations depending on the task. For instance:\r\n",
    "- In `\"CAUSAL_LM\"`, LoRA layers are applied in a manner that respects the autoregressive nature of the task.\r\n",
    "- In `\"SEQ2SEQ_LM\"`, both encoder and decoder modules may be adapted.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Custom/Library-Specific Extensions**\r\n",
    "Some implementations of LoRA or similar libraries may add other task types or extend the list. To ensure compatibility, always refer to the latest documentation of the `peft` library or framework you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae42282-9001-4693-9fa9-bf25a8034286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Achieve higher compression with lower values\n",
    "rank_dimension = 6\n",
    "\n",
    "# Scaling factor\n",
    "lora_alpha = 8\n",
    "\n",
    "# Prevents overfitting - 5%\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    # Rank dimension - between (4,32)\n",
    "    r = rank_dimension,\n",
    "\n",
    "    # Scaling factor - a good starting point is (2 x r)\n",
    "    lora_alpha=lora_alpha,\n",
    "\n",
    "    # Dropout probability for LoRA layers\n",
    "    lora_dropout=lora_dropout,\n",
    "\n",
    "    # Can be ‘none’, ‘all’ or ‘lora_only’\n",
    "    bias = \"none\",\n",
    "\n",
    "    # The names of the modules to apply the adapter to. If this is specified, only the modules with the specified names will be replaced. \n",
    "    target_modules = \"all-linear\",\n",
    "\n",
    "    # Task type for model architecture\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bec392-4b95-49ed-ad98-3444497ccbc5",
   "metadata": {},
   "source": [
    "### 3.2 Trainer configuration\n",
    "\n",
    "\n",
    "This configuration defines a set of hyperparameters and settings for fine-tuning a model using **Supervised Fine-Tuning (SFT)** with recommendations inspired by the **QLoRA paper**. QLoRA is designed for efficient fine-tuning of large language models, leveraging optimizations such as quantization and memory-saving techniques.\r\n",
    "\r\n",
    "Here’s a detailed explanation of each parameter:\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **1. Output Settings**\r\n",
    "- **`output_dir=finetune_name`**: \r\n",
    "  - Specifies the directory where model checkpoints and outputs will be saved. The variable `finetune_name` should hold the name of the fine-tuning task or model.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **2. Training Duration**\r\n",
    "- **`num_train_epochs=1`**:\r\n",
    "  - Defines the number of full passes through the dataset during training. \r\n",
    "  - A single epoch is recommended in QLoRA to avoid overfitting when using large datasets and pre-trained models.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **3. Batch Size Settings**\r\n",
    "- **`per_device_train_batch_size=2`**:\r\n",
    "  - The number of training samples processed per GPU during one forward and backward pass.\r\n",
    "  - A small batch size is chosen to save memory, especially for large models.\r\n",
    "\r\n",
    "- **`gradient_accumulation_steps=2`**:\r\n",
    "  - Accumulates gradients over multiple steps before performing a weight update, effectively creating a larger batch size (`effective batch size = batch size × gradient accumulation steps`).\r\n",
    "  - Helps achieve the benefits of larger batch training without requiring as much memory.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **4. Memory Optimization**\r\n",
    "- **`gradient_checkpointing=True`**:\r\n",
    "  - Enables recomputation of intermediate activations during the backward pass instead of storing them, reducing memory usage at the cost of additional computation.\r\n",
    "  - Useful for fine-tuning large models with limited GPU memory.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **5. Optimizer Settings**\r\n",
    "- **`optim=\"adamw_torch_fused\"`**:\r\n",
    "  - Uses a fused implementation of AdamW optimizer for better efficiency and performance on modern hardware.\r\n",
    "  - AdamW is a variant of Adam that includes weight decay, making it a popular choice for transformer models.\r\n",
    "\r\n",
    "- **`learning_rate=2e-4`**:\r\n",
    "  - Learning rate specifies the step size for updating weights during optimization.\r\n",
    "  - The value is chosen based on QLoRA recommendations for fine-tuning large models.\r\n",
    "\r\n",
    "- **`max_grad_norm=0.3`**:\r\n",
    "  - Implements gradient clipping to prevent excessively large gradients, which can destabilize training.\r\n",
    "  - A low value of 0.3 is recommended for fine-tuning pre-trained models.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **6. Learning Rate Schedule**\r\n",
    "- **`warmup_ratio=0.03`**:\r\n",
    "  - Specifies the proportion of the total training steps to gradually increase the learning rate from 0 to the target value (warmup phase).\r\n",
    "  - Helps avoid large updates at the start of training.\r\n",
    "\r\n",
    "- **`lr_scheduler_type=\"constant\"`**:\r\n",
    "  - Maintains a constant learning rate after the warmup phase. Simpler than decay schedules and works well for short fine-tuning tasks.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **7. Logging and Saving**\r\n",
    "- **`logging_steps=10`**:\r\n",
    "  - Logs training metrics (e.g., loss, accuracy) every 10 steps for monitoring progress.\r\n",
    "\r\n",
    "- **`save_strategy=\"epoch\"`**:\r\n",
    "  - Saves model checkpoints at the end of each epoch, ensuring periodic backups without overloading storage.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **8. Precision Settings**\r\n",
    "- **`bf16=True`**:\r\n",
    "  - Uses **bfloat16 (Brain Floating Point)** precision instead of full 32-bit precision to reduce memory usage and speed up computations.\r\n",
    "  - Bfloat16 maintains a wide range of numerical values, making it suitable for training large models.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **9. Integration Settings**\r\n",
    "- **`push_to_hub=False`**:\r\n",
    "  - Disables automatic pushing of the model to the Hugging Face Hub.\r\n",
    "\r\n",
    "- **`report_to=None`**:\r\n",
    "  - Prevents reporting training progress to external tools like TensorBoard or Weights & Biases.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Summary**\r\n",
    "This configuration is tailored for efficient fine-tuning of large language models using QLoRA techniques:\r\n",
    "- **Memory optimization**: Gradient checkpointing and bfloat16 precision.\r\n",
    "- **Learning efficiency**: Warmup and fused AdamW optimizer.\r\n",
    "- **Minimal overfitting**: Single epoch, gradient clipping, and small learning rate.\r\n",
    "- **Practicality**: Saves checkpoints per epoch and limits external reporting/logging. \r\n",
    "\r\n",
    "It is a practical setup for training large models while minimizing computational and memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e250894a-d5ee-446b-b8d9-86f55e9c50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = datetime.now()\n",
    "\n",
    "# Create a readable timestamp\n",
    "formatted_time = current_time.strftime(\"%b-%d-%Y-%H-%M-%S\")\n",
    "\n",
    "# Create a name for the run\n",
    "wandb_run_name = f\"FT_run_{formatted_time}\"\n",
    "\n",
    "# Adjust the model\n",
    "fine_tuned_model_name = f\"fine-tuned-chat-model-lora\"\n",
    "\n",
    "# Model assets output folder\n",
    "model_output_folder = \"c:/temp/sft_output\"\n",
    "\n",
    "# SFTrainer configuration\n",
    "sft_config = SFTConfig(\n",
    "    \n",
    "    ########################\n",
    "    #### Output setting ####\n",
    "    ########################\n",
    "    # Output directory for model assets\n",
    "    # Specifies the directory where model checkpoints and outputs will be saved.\n",
    "    # The variable finetune_name should hold the name of the fine-tuning task or model.\n",
    "    output_dir = model_output_folder,  \n",
    "\n",
    "    ###########################\n",
    "    #### Training duration ####\n",
    "    ###########################\n",
    "    \n",
    "    # Hyperparameter : Number of epochs\n",
    "    num_train_epochs=1,\n",
    "    \n",
    "    # Hyperparameter : Controls maximum number of steps to be executed\n",
    "    # Maximum number of gradient update steps during training.\n",
    "    max_steps=100,  \n",
    "\n",
    "\n",
    "    ####################\n",
    "    #### Batch size ####\n",
    "    ####################\n",
    "    # Set according to your GPU memory capacity\n",
    "    # Number of training samples per device in each batch. Smaller values help fit large models into memory-constrained GPUs.\n",
    "    per_device_train_batch_size=2,  \n",
    "\n",
    "    # Useful for fine-tuning large models with limited GPU memory\n",
    "    # Accumulates gradients over multiple steps before performing a weight update, \n",
    "    # effectively creating a larger batch size (effective batch size = batch size × gradient accumulation steps).\n",
    "    gradient_accumulation_steps=2,\n",
    "\n",
    "\n",
    "    #############################\n",
    "    #### Memory optimization ####\n",
    "    #############################\n",
    "\n",
    "    # Enables recomputation of intermediate activations during the backward pass instead of storing them, \n",
    "    # reducing memory usage at the cost of additional computation. Useful for fine-tuning large models with limited GPU memory\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "\n",
    "    ###########################\n",
    "    #### Optimizer setting ####\n",
    "    ###########################\n",
    "    # AdamW is a variant of Adam that includes weight decay\n",
    "    optim=\"adamw_torch_fused\",\n",
    "\n",
    "    # Learning rate specifies the step size for updating weights during optimization.\n",
    "    # The initial learning rate for the optimizer.\n",
    "    # Value from QLoRA paper\n",
    "    learning_rate=2e-4,  \n",
    "\n",
    "    # Implements gradient clipping to prevent excessively large gradients, which can destabilize training\n",
    "    # A low value of 0.3 is recommended for fine-tuning pre-trained models.\n",
    "    max_grad_norm=0.3,\n",
    "\n",
    "    ###########################################\n",
    "    #### Learning rate schedule/dynamicity ####\n",
    "    ###########################################\n",
    "\n",
    "    # Specifies the proportion of the total training steps to gradually increase the learning rate from 0 to the target value (warmup phase)\n",
    "    # Helps avoid large updates at the start of training.\n",
    "    warmup_ratio=0.03,\n",
    "\n",
    "    # Maintains a constant learning rate after the warmup phase. Simpler than decay schedules and works well for short fine-tuning tasks.\n",
    "    lr_scheduler_type=\"constant\",\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    #### Evaluatio/validation strategy ####\n",
    "    #######################################\n",
    "\n",
    "    # Evaluate every N steps\n",
    "    eval_strategy=\"steps\",\n",
    "\n",
    "    # Reload the best model at the end of training\n",
    "    # load_best_model_at_end=True,  \n",
    "\n",
    "\n",
    "    ##########################\n",
    "    #### Logging & saving ####\n",
    "    ##########################\n",
    "\n",
    "    # Frequency of logging training metrics\n",
    "    # Logs metrics (e.g., loss) every 10 steps during training.\n",
    "    logging_steps=10,  \n",
    "\n",
    "    # Saves model checkpoints at the end of each epoch, ensuring periodic backups without overloading storage.\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    #### Precision setting ####\n",
    "    ###########################\n",
    "\n",
    "    # Uses bfloat16 (Brain Floating Point) precision instead of full 32-bit precision to reduce memory usage and speed up computations.\n",
    "    bf16=True,\n",
    "\n",
    "    ##############################\n",
    "    #### Integration settings ####\n",
    "    ##############################\n",
    "\n",
    "    # Disables automatic pushing of the model to the Hugging Face Hub\n",
    "    push_to_hub=False,\n",
    "\n",
    "    # Prevents reporting training progress to external tools like TensorBoard or Weights & Biases\n",
    "    report_to=\"wandb\",  # None for disabling reporting\n",
    "\n",
    "    # Set this if you enable wandb\n",
    "    run_name = wandb_run_name,\n",
    "\n",
    "    # Set this if you enable HF Hub push\n",
    "    # Set a unique name for your model - used for HuggingFace hub\n",
    "    hub_model_id=fine_tuned_model_name,  \n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85fbfe-7369-4907-a887-b95457866bc8",
   "metadata": {},
   "source": [
    "## 3. Setup the Supervised Fine-tuning trainer with LoRA\n",
    "\n",
    "**SFTrainer**\n",
    "\n",
    "https://huggingface.co/docs/trl/v0.12.2/en/sft_trainer#trl.SFTTrainer\n",
    "\n",
    "**SFTrainer extends the transformers.Trainer class**\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/trainer#api-reference%20][%20transformers.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "519503b8-43a8-4e9b-a9db-ecd0f9ae050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5320d648c0fe46189bf5396210d8b542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b500de2cc04471190a4e428b8cde213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:428: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SFTTrainer\n",
    "\n",
    "\n",
    "# determines the maximum number of tokens allowed in an input sequence.\n",
    "# If a sequence exceeds this length, it is truncated (or padded if shorter).\n",
    "# This affects both the computational efficiency and memory usage during training and evaluation.\n",
    "max_seq_length = 1512  # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "\n",
    "    # The language model being fine-tuned.\n",
    "    model=model,\n",
    "\n",
    "    # Passes the fine-tuning configuration defined above \n",
    "    args=sft_config,\n",
    "\n",
    "    # Training dataset\n",
    "    train_dataset=ds[\"train\"],\n",
    "\n",
    "    # Evaluation dataset\n",
    "    eval_dataset=ds[\"test\"],\n",
    "\n",
    "    # Tokenizer used\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    # Maximum sequence length\n",
    "    max_seq_length=max_seq_length,\n",
    "\n",
    "    # Enables input packing, which combines multiple short sequences into a single batch to maximize GPU utilization and training efficiency.\n",
    "    packing=True,\n",
    "\n",
    "    # PEFT configuration\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcb839-5071-4eec-9a5e-f7fa79c52266",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "\n",
    "Wandb - configuration\n",
    "https://docs.wandb.ai/guides/track/environment-variables/\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a65199f1-ff8f-4a38-adb2-ffe6112f9022",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./temp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_tuned_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2125\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2126\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2128\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\accelerate\\accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2248\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./temp/{fine_tuned_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69f3b6-face-4159-8dd9-a9903cbfdb08",
   "metadata": {},
   "source": [
    "## 5. Upload to HF hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d175da-043d-4eae-8b55-d10f064fb692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide the HUGGINGFACEHUB_API_TOKEN:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8646bcfdd4346458c2ea5237f5195a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed25f690d9c24e4bb06ac50947012844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa045b5cca04754ab6f61d7986fb011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/acloudfan/fine-tuned-chat-model/commit/e0a163df1518126d0a5d8833a50ef84f45f541fd', commit_message='End of training', commit_description='', oid='e0a163df1518126d0a5d8833a50ef84f45f541fd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/acloudfan/fine-tuned-chat-model', endpoint='https://huggingface.co', repo_type='model', repo_id='acloudfan/fine-tuned-chat-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "print(\"Provide the HUGGINGFACEHUB_API_TOKEN:\")\n",
    "HUGGINGFACEHUB_API_TOKEN=getpass.getpass()\n",
    "\n",
    "trainer.push_to_hub(token=HUGGINGFACEHUB_API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b92a1-5d28-4c10-a60f-78e5d664b302",
   "metadata": {},
   "source": [
    "## 6. Try out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f28d6c-e452-4778-83e4-c599f0f74de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd choose the future. I'd like to see the future, but I don't want to be stuck in the past. What if I had a time machine and could travel back in time? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\n",
    "generator = pipeline(\"text-generation\", model=\"acloudfan/fine-tuned-chat-model\") #, device=\"cuda\")\n",
    "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\n",
    "print(output[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
