{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdd9752-334e-42b4-ac61-c3f0ae3e5a65",
   "metadata": {},
   "source": [
    "# HuggingFace Supervised Fine-tuning Trainer (SFT)\n",
    "## Full Fine-tuning\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer\n",
    "\n",
    "## Gen AI Course Guide\n",
    "https://genai.acloudfan.com/\n",
    "\n",
    "## TinyLlamma\n",
    "https://arxiv.org/pdf/2401.02385\n",
    "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1\n",
    "https://huggingface.co/facebook/opt-350m\n",
    "https://huggingface.co/facebook/MobileLLM-125M\n",
    "\n",
    "## Example scripts\n",
    "https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py\n",
    "\n",
    "## Inspired by\n",
    "https://colab.research.google.com/github/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb\n",
    "\n",
    "\n",
    "## Colab\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/acloudfan/gen-ai-app-dev/blob/main/Fine-Tuning/full-fine-tuning-SFTTrainer.ipynb\">\r\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" />\r\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4606f3-a8fd-47b2-8f09-7ba419be1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354e58d2-84a8-43df-9cd1-52bd4ca35a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816005c1-8134-4026-9f43-b722b0e800b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Select the base model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Requires code to be executed for loading the model\n",
    "# model_name = \"facebook/MobileLLM-125M\"\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"fb-opt-350-ft\"\n",
    "os.environ[\"WANDB_DIR\"] = \"./temp\"\n",
    "os.environ[\"WANDB_JOB_NAME\"] = \"some-job-name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0b81e-df31-45b3-8176-6c3807918ef6",
   "metadata": {},
   "source": [
    "## 1. Load the model to appropriate available device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcad2466-9fd4-4fff-a976-b6d85984575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Check the machine in use and set the device to use for training\n",
    "# cuda = GPU, mps = Metal Performance Shaders on macOS or Apple GPU, cpu otherwise\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print device info\n",
    "print(\"Model loaded to: \", device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pretrained model & move it to the specified device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Setup for the model specific chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbab373-c83f-4902-b4d4-ffa2f10e5e47",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "**Dataset format support**\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfa2555-5797-4015-8084-909f7e63150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"HuggingFaceTB/smoltalk\"\n",
    "dataset_split = \"everyday-conversations\"\n",
    "\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf3b85-9c9f-4edf-9796-3ac29d573819",
   "metadata": {},
   "source": [
    "## 3. Setup the training configuration\n",
    "\n",
    "**SFTConfig**\n",
    "\n",
    "https://huggingface.co/docs/trl/v0.12.2/en/sft_trainer#trl.SFTConfig\n",
    "\n",
    "This object specifies hyperparameters and settings for the fine-tuning process. It’s tailored to supervised fine-tuning tasks, often used for adapting language models to specific tasks or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e250894a-d5ee-446b-b8d9-86f55e9c50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = datetime.now()\n",
    "\n",
    "# Create a readable timestamp\n",
    "formatted_time = current_time.strftime(\"%b-%d-%Y-%H-%M-%S\")\n",
    "\n",
    "# Create a name for the run\n",
    "wandb_run_name = f\"FT_run_{formatted_time}\"\n",
    "\n",
    "# Adjust the model\n",
    "fine_tuned_model_name = f\"fine-tuned-chat-model\"\n",
    "\n",
    "# Model assets output folder\n",
    "model_output_folder = \"c:/temp/sft_output\"\n",
    "\n",
    "# SFTrainer configuration\n",
    "sft_config = SFTConfig(\n",
    "\n",
    "    # Output directory for model assets\n",
    "    output_dir = model_output_folder,  \n",
    "\n",
    "    # Hyperparameter : Controls maximum number of steps to be executed\n",
    "    # Maximum number of gradient update steps during training.\n",
    "    max_steps=100,  \n",
    "\n",
    "    # Common starting point for fine-tuning\n",
    "    # The initial learning rate for the optimizer.\n",
    "    learning_rate=5e-5,  \n",
    "\n",
    "    # Set according to your GPU memory capacity\n",
    "    # Number of training samples per device in each batch. Smaller values help fit large models into memory-constrained GPUs.\n",
    "    per_device_train_batch_size=4,  \n",
    "\n",
    "    # Frequency of logging training metrics\n",
    "    # Logs metrics (e.g., loss) every 10 steps during training.\n",
    "    logging_steps=10,  \n",
    "\n",
    "    # Frequency of saving model checkpoints\n",
    "    # Saves model checkpoints every 100 steps. In case of failure, loss or work will be limited to a maximum of 100 steps\n",
    "    save_steps=100,  \n",
    "\n",
    "    # Evaluate the model at regular intervals\n",
    "    eval_strategy=\"steps\",  \n",
    "\n",
    "    # Frequency of evaluation\n",
    "    # Run the model evaluation after every 50 steps\n",
    "    eval_steps=50,  \n",
    "\n",
    "    # Use MPS for mixed precision training\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  \n",
    "\n",
    "    # Set a unique name for your model - used for HuggingFace hub\n",
    "    hub_model_id=fine_tuned_model_name,  \n",
    "\n",
    "    # Reporting\n",
    "    report_to = \"wandb\",\n",
    "    run_name = wandb_run_name,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85fbfe-7369-4907-a887-b95457866bc8",
   "metadata": {},
   "source": [
    "## 3. Setup the Supervised Fine-tuning traine\n",
    "\n",
    "**SFTrainer**\n",
    "\n",
    "https://huggingface.co/docs/trl/v0.12.2/en/sft_trainer#trl.SFTTrainer\n",
    "\n",
    "**SFTrainer extends the transformers.Trainer class**\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/trainer#api-reference%20][%20transformers.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "519503b8-43a8-4e9b-a9db-ecd0f9ae050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77942cb069e0466aada74a32dfc256bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082c55b709f54c229545f0ecf9e9c1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "\n",
    "    # The language model being fine-tuned.\n",
    "    model=model,\n",
    "\n",
    "    # Passes the fine-tuning configuration defined above \n",
    "    args=sft_config,\n",
    "\n",
    "    # Training dataset\n",
    "    train_dataset=ds[\"train\"],\n",
    "\n",
    "    # Evaluation dataset\n",
    "    eval_dataset=ds[\"test\"],\n",
    "\n",
    "    # Tokenizer used\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcb839-5071-4eec-9a5e-f7fa79c52266",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "\n",
    "Wandb - configuration\n",
    "https://docs.wandb.ai/guides/track/environment-variables/\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65199f1-ff8f-4a38-adb2-ffe6112f9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: WARNING Path ./temp\\wandb\\ wasn't writable, using system temp directory\n",
      "wandb: Currently logged in as: acloudfan (raj-acloudfan). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING Path ./temp\\wandb\\ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\raj\\AppData\\Local\\Temp\\wandb\\run-20241212_173844-w8d19txu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raj-acloudfan/fb-opt-350-ft/runs/w8d19txu' target=\"_blank\">FT_run_Dec-12-2024-17-38-40</a></strong> to <a href='https://wandb.ai/raj-acloudfan/fb-opt-350-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raj-acloudfan/fb-opt-350-ft' target=\"_blank\">https://wandb.ai/raj-acloudfan/fb-opt-350-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raj-acloudfan/fb-opt-350-ft/runs/w8d19txu' target=\"_blank\">https://wandb.ai/raj-acloudfan/fb-opt-350-ft/runs/w8d19txu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 20:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.306700</td>\n",
       "      <td>1.389864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.417900</td>\n",
       "      <td>1.327439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{fine_tuned_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69f3b6-face-4159-8dd9-a9903cbfdb08",
   "metadata": {},
   "source": [
    "## 5. Upload to HF hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d175da-043d-4eae-8b55-d10f064fb692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide the HUGGINGFACEHUB_API_TOKEN:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8646bcfdd4346458c2ea5237f5195a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed25f690d9c24e4bb06ac50947012844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa045b5cca04754ab6f61d7986fb011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/acloudfan/fine-tuned-chat-model/commit/e0a163df1518126d0a5d8833a50ef84f45f541fd', commit_message='End of training', commit_description='', oid='e0a163df1518126d0a5d8833a50ef84f45f541fd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/acloudfan/fine-tuned-chat-model', endpoint='https://huggingface.co', repo_type='model', repo_id='acloudfan/fine-tuned-chat-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "print(\"Provide the HUGGINGFACEHUB_API_TOKEN:\")\n",
    "HUGGINGFACEHUB_API_TOKEN=getpass.getpass()\n",
    "\n",
    "trainer.push_to_hub(token=HUGGINGFACEHUB_API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b92a1-5d28-4c10-a60f-78e5d664b302",
   "metadata": {},
   "source": [
    "## 6. Try out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f28d6c-e452-4778-83e4-c599f0f74de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd choose the future. I'd like to see the future, but I don't want to be stuck in the past. What if I had a time machine and could travel back in time? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person? Would I still be the same person?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\n",
    "generator = pipeline(\"text-generation\", model=\"acloudfan/fine-tuned-chat-model\") #, device=\"cuda\")\n",
    "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\n",
    "print(output[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
