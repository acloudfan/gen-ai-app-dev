{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ff2804-42fe-4edf-931b-6dc30a0dd062",
   "metadata": {},
   "source": [
    "# Multi label classification - toxicity classification\n",
    "\n",
    "Inspired by the following:\n",
    "\n",
    "https://colab.research.google.com/drive/1aue7x525rKy6yYLqqt-5Ll96qjQvpqS7\n",
    "\n",
    "**Additional datasets to try**\n",
    "\n",
    "Twitter News : Financial categorization\n",
    "https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic\n",
    "\n",
    "**Learnings:**\n",
    "\n",
    "1. Prepare datasets for multi label classification\n",
    "2. Learn about evaluate function\n",
    "3. Effect of hyperparameters on evaluation loss\n",
    "4. Use of custom metric for evaluation\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1881d2-0240-403e-866e-09b39a60e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on transformers v4.46.3 and datasets v3.1.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(f\"Running on transformers v{transformers.__version__} and datasets v{datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d645e1ed-9958-45bd-b2b9-3a10184b14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e55cf3-332c-4a2e-9d71-e64d01cdf0ba",
   "metadata": {},
   "source": [
    "## 1. Load dataset\n",
    "\n",
    "Loads a very small demo dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d12eb06-e7cf-481b-9230-161711b1a2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': '\"\"\"Nazi filth\"\" is impolite  04:27, 20 Jan 2004 (UTC)\\n\\n\"',\n",
       " 'toxic': 1,\n",
       " 'threat': 0,\n",
       " 'insult': 1,\n",
       " 'identity_hate': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"acloudfan/toxicity-multi-label-classifier\"\n",
    "\n",
    "# Number of labels in the dataset ['toxic', 'threat', 'insult', 'identity_hate']\n",
    "NUM_LABELS=4\n",
    "\n",
    "# Load\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "# Show one row\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b424a-4202-4627-a041-3426e9477628",
   "metadata": {},
   "source": [
    "## 2. Create labels column\n",
    "\n",
    "Create a column named labels with *One Hot Encoding*.\n",
    "\n",
    "Trainer requires the **labels** column with all classes encoded in a list such as [1,0,1,0]\n",
    "\n",
    "At the end of this we will have an additional column with 1's and 0s for applicable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c342cd3c-cfbd-4b7a-902d-e06a9ef546e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment_text', 'toxic', 'threat', 'insult', 'identity_hate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'threat', 'insult', 'identity_hate', 'labels'],\n",
       "        num_rows: 89\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'threat', 'insult', 'identity_hate', 'labels'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'threat', 'insult', 'identity_hate', 'labels'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create labels column\n",
    "cols = dataset[\"train\"].column_names\n",
    "\n",
    "# Print column names\n",
    "print(cols)\n",
    "\n",
    "# Function for pre-processing the labels column\n",
    "def pre_process_ohc(sample):\n",
    "    labels = []\n",
    "\n",
    "    # Create the labels array\n",
    "    for col in cols:\n",
    "        if col != \"comment_text\":\n",
    "            labels.append(sample[col])\n",
    "    \n",
    "    sample[\"labels\"]=labels\n",
    "    return sample\n",
    "\n",
    "# Call map on dataset to pre-process\n",
    "dataset_pre_processed = dataset.map(pre_process_ohc)\n",
    "\n",
    "dataset_pre_processed\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892e883-de1b-4abe-a13c-dc3c8d6eeeec",
   "metadata": {},
   "source": [
    "## 3. Tokenize\n",
    "\n",
    "Pay attention to the *problem_type*, this leads to the selection of the loss function that applies to multi-label classification. Cross entropy is used instead of MSE. After tokenization, we need to remove all columns expcept [labels, input_ids, attention_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849e9915-5d51-4e8b-9251-cae56d3eaeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try out other models\n",
    "MODEL_ID = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7dcf5e2-ba6c-4486-9bfd-0e317fd58a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder function\n",
    "def tokenize_and_encode(examples):\n",
    "  return tokenizer(examples[\"comment_text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb1e561-17fc-4161-94ce-61eaf9ef0c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 89\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare list of columns, that will be deleted - labels column will be kept\n",
    "cols = dataset_pre_processed[\"train\"].column_names\n",
    "cols.remove(\"labels\")\n",
    "\n",
    "# Tokenize, Encode and remove columns\n",
    "ds_enc = dataset_pre_processed.map(tokenize_and_encode, batched=True, remove_columns=cols)\n",
    "ds_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4868bc1-9aac-479f-a0b4-86132c34b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch requires labels to be Float, cast labels to floats\n",
    "ds_enc.set_format(\"torch\")\n",
    "\n",
    "# Create a new column 'float_labels' to hold the labels, remove labels column, rename float_labels to labels\n",
    "ds_enc = (ds_enc\n",
    "          .map(lambda x : {\"float_labels\": x[\"labels\"].to(torch.float)}, remove_columns=[\"labels\"])\n",
    "          .rename_column(\"float_labels\", \"labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694dff8b-eee0-40f6-845e-7491b1451317",
   "metadata": {},
   "source": [
    "## 4. Load model\n",
    "\n",
    "Load the model for training using the AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0b20b4b-c8de-4e15-a2ca-40dddcce0c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels=4\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=num_labels, problem_type=\"multi_label_classification\") #.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69fcad-37c7-4697-bba0-b1ed5c21819e",
   "metadata": {},
   "source": [
    "## 5. Create Trainer & Evaluate\n",
    "\n",
    "You may change training arguments to make the model perform better and see its impact on evaluation loss.\n",
    "* Change the epoch to 2, 3, 4, 5  and observe the eval loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b96b80f-3606-485e-9af7-b2130e8968d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\AppData\\Local\\Temp\\ipykernel_43248\\1720953040.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=args, train_dataset=ds_enc[\"train\"], eval_dataset=ds_enc[\"test\"], tokenizer=tokenizer)\n"
     ]
    }
   ],
   "source": [
    "# ds_enc[\"train\"][0]\n",
    "\n",
    "# Adjust number of epochs and see the change in loss\n",
    "eval_strategy='epoch'\n",
    "num_train_epochs=5\n",
    "\n",
    "# Set if eval strategy is steps\n",
    "eval_steps = 5\n",
    "\n",
    "# Create the training arguments\n",
    "args = TrainingArguments(\"./temp\", num_train_epochs=num_train_epochs,  report_to='none', eval_strategy=eval_strategy, eval_steps=eval_steps)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=ds_enc[\"train\"], eval_dataset=ds_enc[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cfff5bd-d5f3-48a8-a10d-cbad7ef97611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7024906277656555,\n",
       " 'eval_model_preparation_time': 0.0032,\n",
       " 'eval_runtime': 3.833,\n",
       " 'eval_samples_per_second': 9.131,\n",
       " 'eval_steps_per_second': 1.304}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a297ef4-3983-4db7-a82e-35d756a9ff93",
   "metadata": {},
   "source": [
    "## 6. Run fine tuning & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5b63d8-3abd-4ac3-918f-17237c09c073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.582331</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562377</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539277</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.517577</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501184</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.495507</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.5028909365336101, metrics={'train_runtime': 212.8337, 'train_samples_per_second': 2.091, 'train_steps_per_second': 0.282, 'total_flos': 23880687587568.0, 'train_loss': 0.5028909365336101, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efc823a-5c38-46d6-a3e6-88f80aec5cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4955069422721863,\n",
       " 'eval_model_preparation_time': 0.0032,\n",
       " 'eval_runtime': 2.1477,\n",
       " 'eval_samples_per_second': 16.296,\n",
       " 'eval_steps_per_second': 2.328,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e6011-34ee-462c-919d-ecc33e758b2b",
   "metadata": {},
   "source": [
    "## 7. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edcfa1e1-6a01-45d9-8fde-0ca9764ab528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_labels_with_scores(model, text):\n",
    "    \"\"\"\n",
    "    Predicts confidence scores for all labels for a given text using a fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        model: The fine-tuned multi-label classification model.\n",
    "        text (str): The input text to predict labels for.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of confidence scores for all labels.\n",
    "              Example: [0.95, 0.12, 0.87, 0.45] for 4 labels.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    print(inputs)\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass: get the logits from the model\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits (raw predictions) from the model\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Apply sigmoid to convert logits to probabilities (for multi-label classification)\n",
    "    probabilities = torch.sigmoid(logits).squeeze().tolist()\n",
    "    \n",
    "    # If probabilities is a single value (e.g., for single-sample inputs), convert it to a list\n",
    "    if isinstance(probabilities, float):\n",
    "        probabilities = [probabilities]\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `model` is your fine-tuned multi-label classification model and `text` is the input text\n",
    "# predicted_scores = predict_labels_with_scores(model, \"Your input text here\")\n",
    "# print(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6a8ca60-8d26-435f-b166-65bd777799a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2097, 3102, 2017,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7324583530426025,\n",
       " 0.6496965885162354,\n",
       " 0.28404712677001953,\n",
       " 0.2604326605796814]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id2label={'0':'toxic', '1':'threat', '2':'insult', '3':'identity_hate'}\n",
    "predict_labels_with_scores(trainer.model,\"I will kill you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02cafd9-4c36-48c2-ad0d-bcfc3c84a8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
