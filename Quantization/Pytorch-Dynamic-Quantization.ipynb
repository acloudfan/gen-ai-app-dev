{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73722457-251a-4a74-a7ef-5334277801b4",
   "metadata": {},
   "source": [
    "# PyTorch quantization\n",
    "\n",
    "PyTorch quantization is a technique used to optimize deep learning models by reducing their numerical precision, which helps decrease memory usage and improve inference speed. It encompasses several methods, including **static and dynamic quantization**. \n",
    "\n",
    "Static quantization involves quantizing both weights and activations prior to model deployment, often requiring calibration with representative data. In contrast, dynamic quantization focuses on quantizing only the model's weights at inference time, while the activations remain in floating-point format. This flexibility allows developers to tailor quantization strategies based on specific use cases, ultimately enhancing the performance of models, particularly on resource-constrained environments like mobile devices and edge computing platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa2edf-2efa-49e3-a92d-b6ed95617f86",
   "metadata": {},
   "source": [
    "## Pytorch dynamic quantization\n",
    "\n",
    "* You may note a marked reduction in accuracy for some models\n",
    "* INT4 not supported\n",
    "* Model will be downloaded to cache (may take a few minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86249509-07be-48c6-a613-270861c2e697",
   "metadata": {},
   "source": [
    "### 1. Load model and invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca17b56-cf21-4272-90dc-9e5700426dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Output:\n",
      "Once upon a time, I was a student at the University of California,\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer (e.g., GPT-2)\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Test the model before quantization\n",
    "text = \"Once upon a time,\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "# Print model output\n",
    "print(\"Original Model Output:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411318d-71df-4ee6-81b5-1d03000085de",
   "metadata": {},
   "source": [
    "### 2. Apply dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36697263-ee23-4297-99b1-dd288b0c5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized Model Output:\n",
      "Once upon a time, I was a student at the University of California,\n"
     ]
    }
   ],
   "source": [
    "# Apply dynamic quantization to the model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # the model to quantize\n",
    "    {torch.nn.Linear},  # layers to quantize (focusing on Linear layers)\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Test the quantized model's output\n",
    "outputs_quantized = quantized_model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "# Print the output\n",
    "print(\"\\nQuantized Model Output:\")\n",
    "print(tokenizer.decode(outputs_quantized[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a901d8b-5060-47fb-a664-5ccbe5f3e3c6",
   "metadata": {},
   "source": [
    "### 3. Compare the sizes of original & quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07030a92-d30b-4d9c-877b-f207a794d676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model size of Original_Model: 501.03 MB\n",
      "\n",
      "Model size of Quantized_Model: 284.89 MB\n"
     ]
    }
   ],
   "source": [
    "# Function to print and compare model sizes\n",
    "# Code below serializes the model to the file system\n",
    "# Note that this is just to get an idea of relatives sizes\n",
    "# and not the exact memory footprints.\n",
    "def print_size_of_model(model, model_name=\"\"):\n",
    "    torch.save(model.state_dict(), f\"{model_name}.pt\")\n",
    "    size_mb = os.path.getsize(f'{model_name}.pt') / 1e6\n",
    "    print(f\"\\nModel size of {model_name}: {size_mb:.2f} MB\")\n",
    "\n",
    "# Compare sizes of original and quantized models\n",
    "print_size_of_model(model, \"Original_Model\")\n",
    "print_size_of_model(quantized_model, \"Quantized_Model\")\n",
    "\n",
    "# Clean up saved files after checking size\n",
    "os.remove(\"Original_Model.pt\")\n",
    "os.remove(\"Quantized_Model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aabcf0-2f9e-46af-a374-125e464fec57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
