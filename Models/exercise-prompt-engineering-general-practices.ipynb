{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03941d",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "## General Best Practices\n",
    "\n",
    "1. Separation between parts\n",
    "2. Detailed, clear & specific instructions\n",
    "3. Avoid ambiguity \n",
    "4. Favor positive instructions\n",
    "5. Instruct the model to play a role\n",
    "6. Provide trusted information\n",
    "7. Specify response characteristics\n",
    "8. Set guardrails for response\n",
    "\n",
    "**Note**\n",
    "\n",
    "* You may run into 503 issues, wait for some time and try again\n",
    "* Use a different model in case of 503 that doesnt get resolved\n",
    "* Keep in mind models may behave differently !! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01653920-1e89-4167-a837-3853db51a45c",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the HF API Token\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7f222-566c-47e9-85ff-67ab58afbf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The script is downloaded and run to setup the utils folder\n",
    "\n",
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2c946",
   "metadata": {},
   "source": [
    "## Change the location of the environment file before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc026b9-a5d8-4f38-90e1-241a7ce57fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d152a6",
   "metadata": {},
   "source": [
    "## Setup the models available for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'google/flan-t5-xxl',\n",
    "    'google/gemma-2-2b-it'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8f096",
   "metadata": {},
   "source": [
    "## 1. Guide the model to avoid hallucinations\n",
    "\n",
    "Add the guidance to not to make up a response.\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[2]    \n",
    "\n",
    "'openlm-research/open_llama_3b_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be60250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_bad  = \"\"\"as of january 2024, who is the prime minister of UK\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "## Using inference client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[2])\n",
    "# llm.text_generation(prompt_bad)\n",
    "\n",
    "## HTTP Post\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[2])\n",
    "llm_client.invoke(prompt_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685a1c9",
   "metadata": {},
   "source": [
    "## 2. Instruct the model to play roles\n",
    "\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[3],\n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de27503",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"question:what are large language models. answer:\"\n",
    "\n",
    "## Using inference client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "# llm.text_generation(prompt, max_new_tokens=100)\n",
    "\n",
    "## HTTP Post\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[3])\n",
    "llm_client.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebf2f0",
   "metadata": {},
   "source": [
    "## 3. Guide the model to generate precise response\n",
    "\n",
    "### Default model\n",
    "hugging_face_model_ids[3], \n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2061bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_samples = [\n",
    "    \"how much would be the cost of 'Digital Camera DSLR'.\",\n",
    "    \"share details of home security camera\",\n",
    "    \"do you sell ceiling fan?\",\n",
    "    \"share details of Robot Cleaner?\",\n",
    "]\n",
    "\n",
    "## Using Inference Client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "# no_context_response = llm.text_generation(prompt_samples[0])\n",
    "# print(\"Response without context: \", no_context_response)\n",
    "\n",
    "## HTTP Post call\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[2])\n",
    "no_context_response = llm_client.invoke(prompt_samples[0])\n",
    "print(\"Response without context: \", no_context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b5e8e",
   "metadata": {},
   "source": [
    "## 4. Address bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b186e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"for the engineering jobs we should only hire men 100% of the time, do you agree? \"\n",
    "\n",
    "\n",
    "# Change the index to try out different models\n",
    "## Inference Client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  \n",
    "# without_guardrails = llm.text_generation(prompt)\n",
    "# print(\"Response without guardrails: \", without_guardrails)\n",
    "\n",
    "## HTTP Post call\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[4])\n",
    "without_guardrails = llm_client.invoke(prompt)\n",
    "print(\"Response without guardrails: \", without_guardrails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84035d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
