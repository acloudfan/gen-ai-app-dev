{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684d20c8-64a4-4cba-869c-5caae4be165f",
   "metadata": {},
   "source": [
    "# Stuff documents & Retrieval pipeline\n",
    "\n",
    "Uses the *stuff documents chain* and *retrieval chain* to setup a basic RAG Q&A pipeline.\n",
    "\n",
    "1. Setup a Vector database (Chroma)\n",
    "2. Setup a prompt\n",
    "3. Create the RAG chain as a combination of:\n",
    "   - Stuff document chain\n",
    "   - Retrival chain\n",
    "5. Test & Demonstrates the challenge with the retrieval of context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97424890-7c4e-4d0d-b5da-d99a96a7a5c9",
   "metadata": {},
   "source": [
    "## Setup LLM\n",
    "\n",
    "Use the utility script for creating an instance of the desired LLM object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29ef1da-a8a0-4ab7-8b4d-434ed29cb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "llm = create_gpt_chat_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10246fb1-2847-4d75-87e4-3f50667aea63",
   "metadata": {},
   "source": [
    "## 1. Setup vector database\n",
    "\n",
    "* Use the WebBaseLoader to load a few blogs\n",
    "* Chunk the content of the blogs\n",
    "* Add the chunks to ChrobaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05f80fe-d966-4e2d-a3cd-8dd42128bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a couple of Blogs \n",
    "\n",
    "\n",
    "# Sample blogs on RAG that we will add to vector database\n",
    "\n",
    "# RAG, Tuning & Scratch build techniques\n",
    "url1 = \"https://cloud.google.com/blog/products/ai-machine-learning/to-tune-or-not-to-tune-a-guide-to-leveraging-your-data-with-llms\"\n",
    "\n",
    "# Discusses how Agents can be used with Amazon models\n",
    "url2 = \"https://aws.amazon.com/blogs/aws/build-rag-and-agent-based-generative-ai-applications-with-new-amazon-titan-text-premier-model-available-in-amazon-bedrock/\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url1,url2)\n",
    ")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef81260-3c48-4fc0-a5a7-f4ae653321a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chunk the blogs\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159e29a9-e930-4155-92ee-6ebb8aa46fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1f627d65390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Add chunks to the ChromaDB\n",
    "\n",
    "# load it into Chroma using default embedding all-MiniLM-L6-v2\n",
    "collection_name = 'sample-blog'\n",
    "collection_metadata = {'embedding': 'all-MiniLM-L6-v2'}\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma(collection_name=collection_name, collection_metadata=collection_metadata, embedding_function=embedding_function)\n",
    "vector_store.add_documents(chunked_documents)\n",
    "\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e2507-8bc2-438e-ac35-3e9df40870bb",
   "metadata": {},
   "source": [
    "## 2. Setup the prompt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e73614-8db1-426c-8474-1d59e770f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee75ea4-bff0-4c98-9ab6-3dd7b62264eb",
   "metadata": {},
   "source": [
    "## 3. Create the RAG chain\n",
    "\n",
    "https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html\n",
    "\n",
    "https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2658d07-9382-4336-b3ce-e095d8e8016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Retriever gets invoked with the question text and passed to the retriever\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c9798-0439-4aa2-a272-879bf7db9b9d",
   "metadata": {},
   "source": [
    "## 4. Demo : Challenge with retrievers !!!\n",
    "\n",
    "The chain we have built uses the input query directly to retrieve relevant context. But in a conversational setting, the user query might require conversational context to be understood. This challenge is demonstrated via the following inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5331373-2497-4265-9455-d9eb4572c1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG, or Retrieval-augmented generation, helps ensure model outputs are grounded on your data by searching for relevant information in your data for a query and passing that information into the prompt. It supports fresh, constantly updated data, private data, large-scale multimodal data, and more, with an ecosystem of products that enable its implementation. RAG is beneficial for controlling access to grounding data, combating hallucinations, and enhancing result interpretability.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is RAG?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a9e1d5-02df-4e9c-8498-7b3c98e560ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fine-tuning involves training a model on specific input-output pairs provided by the user to adapt it to a particular task, such as classifying meeting transcripts into categories. On the other hand, RLHF, or Reinforcement Learning from Human Feedback, focuses on tuning a model based on human preferences for desired behavior that may not be easily quantifiable or categorized, such as achieving a specific tone or brand voice. While fine-tuning requires labeled data for supervised learning, RLHF relies on human feedback to guide the model's adjustments towards the desired outcome.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"How is it different than fine tuning?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dae39b-9747-49ad-af2f-77c845db3b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
