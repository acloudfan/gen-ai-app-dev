{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c997ac20",
   "metadata": {},
   "source": [
    "# Exercise-2 : Create Wikismall dataset\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "* Create a dataset with 'train' & 'validation' split for experimentation\n",
    "\n",
    "* The total number of rows in the dataset will be 0.01% of the original dataset (wikipedia/20220301.en)\n",
    "\n",
    "**Note:**  This dataset will be used for demonstrating the pre-training of Roberta model\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Load the dataset wikipedia/20220301.en \n",
    "2. Create a split with 0.01% of the original data in the 'train' split\n",
    "   * We will use ONLY the 'test' split i.e., 0.01% of original data\n",
    "3. Pre-process : Break wiki paragraphs into sentences\n",
    "   * Each row in the original dataset is a large text blob (one or more paragraphs)\n",
    "   *  Sentences from the same paragraph have common attributes [id, url, title]\n",
    "   * Do NOT shuffle the dataset\n",
    "4. Split the dataset into [train, validation] with [90%, 10%] split\n",
    "6. Upload dataset to HF   e.g., I pushed it to acloudfan/wikismall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5e22b-6a66-411f-837e-f38868c185bf",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a71e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b69aa-d613-4b70-aa44-9675e4551418",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e2e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb702d2",
   "metadata": {},
   "source": [
    "## 1. Load Dataset \n",
    "\n",
    "Note:\n",
    "\n",
    "It may take over 30 minutes to download the dataset as it is huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6536aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a34130d68348e69503ec2d22866a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fade452360274130a1d1129edbc2db34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd00449d7d843c08c460640d2da2527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fbdedcbaac4436911ee0675ec4a580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download just the english dataset\n",
    "wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\") #, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5e29a",
   "metadata": {},
   "source": [
    "## 2. Split the dataset 'train'\n",
    "\n",
    "Our objective is to create a dataset which is for experimentation.\n",
    "\n",
    "Train = 99.99%\n",
    "\n",
    "Test  = 0.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5364a633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 6407173\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 641\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of the data in 'test' split\n",
    "PCT = 0.0001\n",
    "\n",
    "# split\n",
    "wiki_cut = wiki['train'].train_test_split(test_size=PCT) \n",
    "\n",
    "wiki_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12bfd27",
   "metadata": {},
   "source": [
    "## 3. Pre-Process data\n",
    "\n",
    "Break paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883b995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ntlk is used for breaking the paragraphs into sentences\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer data\n",
    "\n",
    "def paragraph_to_sentences(paragraph):\n",
    "    # Use nltk's sent_tokenize function to split the paragraph into sentences\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53866a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!!!\n"
     ]
    }
   ],
   "source": [
    "# create a pandas dataframe\n",
    "pd_dataset = pd.DataFrame(columns=[\"id\",\"url\",\"title\", \"text\"])\n",
    "\n",
    "# 'small_wiki_mlm'\n",
    "for dat in wiki_cut['test']:\n",
    "    print('.', end =\"\")\n",
    "    # Replace the newlines with ''\n",
    "    text = dat['text'].replace('\\n\\n', '').replace('\\n', '')\n",
    "    \n",
    "    # break paragraph into sentences\n",
    "    sentences = paragraph_to_sentences(text)\n",
    "    \n",
    "    # create the dict\n",
    "    dat['text'] = sentences\n",
    "    df_dictionary = pd.DataFrame(dat)\n",
    "    pd_dataset = pd.concat([pd_dataset, df_dictionary])\n",
    "    \n",
    "print(\"!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1e8d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 9825\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataset from pandas data frame - remove any unneeded columns\n",
    "dataset = Dataset.from_pandas(pd_dataset).remove_columns(['__index_level_0__'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e1118",
   "metadata": {},
   "source": [
    "## 4. Create wikismall dataset with train & validation split\n",
    "\n",
    "Split the dataset \n",
    "\n",
    "Train = 90%\n",
    "\n",
    "Validation = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795b2ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 8842\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 983\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set 10% for test\n",
    "PCT=0.1\n",
    "\n",
    "datasets = DatasetDict()\n",
    "datasets['train'] = dataset\n",
    "datasets = dataset.train_test_split(test_size=PCT,  shuffle=True)\n",
    "\n",
    "# Create a new DatasetDict\n",
    "wikismall = DatasetDict()\n",
    "wikismall['train'] = datasets['train']\n",
    "wikismall['validation']=datasets['test']\n",
    "\n",
    "wikismall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74053d",
   "metadata": {},
   "source": [
    "## 5. Push to hub\n",
    "\n",
    "**Note**\n",
    "\n",
    "CHANGE the HF_TOKEN and the name of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67fb2886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968236e7b5394562a13f5a5e01f535a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33ee54a524c4bd68982ea2c16650a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58556be041c942ddb741b7ce77378ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328da10ee705430f917a6c2fdb28580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0279513845224e6089053459e290b769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/509 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HF_TOKEN='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'\n",
    "\n",
    "DATASET_NAME = \"acloudfan/wikismall\"\n",
    "\n",
    "wikismall.push_to_hub(DATASET_NAME, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1d0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
